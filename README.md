Here are three simple perceptions that had been taught in Udacity's Machine Learning Nanodegree.

Perceptrons contain an activation function (which can be interpreted as some sort of mapping function) that transforms inputs into certain output value based on specific activation rule.

Feel free to change the weights and bias in the Python programs, and see what happens. 
Note that test_inputs array is given, and correct_outputs array comes directly from perceptron activation, which in turn is defined via the weights and bias that we can adjust in these two simple examples.
